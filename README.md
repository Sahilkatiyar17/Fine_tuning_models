Fine-Tuning Models â€“ Repository

A collection of notebooks demonstrating how to fine-tune various models (LLMs, BERT, embeddings, CLIP etc.) for real-world downstream tasks using different techniques such as QLoRA, LoRA, supervised fine-tuning, and retrieval optimization.

This repo is meant as a hands-on learning & practical implementation hub for fine-tuning deep learning models, especially in NLP, Medical domain tasks & Recommendation systems.

âœ¨ What this Repository Contains
Section	Description
-  BERT Fine-Tuning for Text Classification	Finetuning BERT for a classification task (sentiment/topic/medical text etc.). Covers preprocessing, training loop, evaluation metrics.
-  LLM Fine-Tuning using LoRA (Ollama/LLaMA-2)	Parameter-efficient fine-tuning for LLaMA-2 using LoRA on a medical dataset.
-  DeepSeek Fine-Tuning (SFT + QLoRA)	Training DeepSeek model with instruction datasets + unsloth for fast memory-efficient training.
-  Embedding Fine-Tuning	Improving vector representations for downstream retrieval ranking tasks.
-  CLIP Embedding Fine-Tuning for Retrieval (Re-Ranking)	Enhancing CLIP embeddings to improve vector similarity search & RAG quality.

All implementations are in Jupyter Notebooks, step-by-step, beginner-friendly with code + explanation.

ðŸ§  Techniques Used

LoRA (Low-Rank Adaptation)

PEFT â€“ Parameter Efficient Fine-Tuning

Supervised Fine-Tuning (SFT)

Embedding Model Optimization

CLIP Multimodal Embeddings

RAG-Oriented Vector Fine-Tuning

DeepSeek + Unsloth Training
